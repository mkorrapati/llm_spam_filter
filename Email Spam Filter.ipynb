{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Coding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "### Run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!mkdir -p data/spam_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget --no-check-certificate https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\n",
    "!wget --no-check-certificate https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\n",
    "!wget --no-check-certificate https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\n",
    "!wget --no-check-certificate https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\n",
    "!wget --no-check-certificate https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!tar -xjf 20030228_easy_ham.tar.bz2 -C data/spam_data2\n",
    "!tar -xjf 20030228_easy_ham_2.tar.bz2 -C data/spam_data2\n",
    "!tar -xjf 20030228_spam.tar.bz2 -C data/spam_data2\n",
    "!tar -xjf 20050311_spam_2.tar.bz2 -C data/spam_data2\n",
    "!tar -xjf 20030228_hard_ham.tar.bz2 -C data/spam_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.26.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: plotly in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (5.17.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (2023.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.venv/lib/python3.9/site-packages (from plotly->-r requirements.txt (line 5)) (8.2.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 4)) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/spam_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_ham_paths = glob.glob(path+'easy_ham/*')\n",
    "easy_ham_2_paths = glob.glob(path+'easy_ham_2/*')\n",
    "hard_ham_paths = glob.glob(path+'hard_ham/*')\n",
    "spam_paths = glob.glob(path+'spam/*')\n",
    "spam_2_paths = glob.glob(path+'spam_2/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_content(email_path):\n",
    "    file = open(email_path,encoding='latin1')\n",
    "    try:\n",
    "        msg = email.message_from_file(file)\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                return part.get_payload() # prints the raw text\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_content_bulk(email_paths):\n",
    "    email_contents = [get_email_content(o) for o in email_paths]\n",
    "    return email_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_path = [\n",
    "    easy_ham_paths,\n",
    "    easy_ham_2_paths,\n",
    "    hard_ham_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_path = [\n",
    "    spam_paths,\n",
    "    spam_2_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_sample = np.array([train_test_split(o) for o in ham_path], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train = np.array([])\n",
    "ham_test = np.array([])\n",
    "for o in ham_sample:\n",
    "    ham_train = np.concatenate((ham_train,o[0]),axis=0)\n",
    "    ham_test = np.concatenate((ham_test,o[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3112,), (1038,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_train.shape, ham_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sample = np.array([train_test_split(o) for o in spam_path], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train = np.array([])\n",
    "spam_test = np.array([])\n",
    "for o in spam_sample:\n",
    "    spam_train = np.concatenate((spam_train,o[0]),axis=0)\n",
    "    spam_test = np.concatenate((spam_test,o[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1422,), (474,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_train.shape, spam_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train_label = [0]*ham_train.shape[0]\n",
    "spam_train_label = [1]*spam_train.shape[0]\n",
    "x_train = np.concatenate((ham_train,spam_train))\n",
    "y_train = np.concatenate((ham_train_label,spam_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_test_label = [0]*ham_test.shape[0]\n",
    "spam_test_label = [1]*spam_test.shape[0]\n",
    "x_test = np.concatenate((ham_test,spam_test))\n",
    "y_test = np.concatenate((ham_test_label,spam_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffle_index = np.random.permutation(np.arange(0,x_train.shape[0]))\n",
    "test_shuffle_index = np.random.permutation(np.arange(0,x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[train_shuffle_index]\n",
    "y_train = y_train[train_shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[test_shuffle_index]\n",
    "y_test = y_test[test_shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_email_content_bulk(x_train)\n",
    "x_test = get_email_content_bulk(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null(datas,labels):\n",
    "    not_null_idx = [i for i,o in enumerate(datas) if o is not None]\n",
    "    return np.array(datas)[not_null_idx],np.array(labels)[not_null_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = remove_null(x_train,y_train)\n",
    "x_test,y_test = remove_null(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlink(word):\n",
    "    return  re.sub(r\"http\\S+\", \"\", word)\n",
    "\n",
    "def to_lower(word):\n",
    "    result = word.lower()\n",
    "    return result\n",
    "\n",
    "def remove_number(word):\n",
    "    result = re.sub(r'\\d+', '', word)\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result\n",
    "\n",
    "def remove_whitespace(word):\n",
    "    result = word.strip()\n",
    "    return result\n",
    "\n",
    "def replace_newline(word):\n",
    "    return word.replace('\\n','')\n",
    "\n",
    "def clean_up_pipeline(sentence):\n",
    "    cleaning_utils = [remove_hyperlink,\n",
    "                      replace_newline,\n",
    "                      to_lower,\n",
    "                      remove_number,\n",
    "                      remove_punctuation,remove_whitespace]\n",
    "    for o in cleaning_utils:\n",
    "        sentence = o(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_up_pipeline(o) for o in x_train]\n",
    "x_test = [clean_up_pipeline(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitbitchmagnesiumnet wroteim also going to also rescind my prior request not to have a memberonlyfork   as snobby and elitist as a memberonly fork would be im startingto feel the pain of the whole spam to bits ratio on forkhere here the memberonly lists that im on have a spam count of near zero an additional benefit is that the list archives arent full of spam eithere\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(x_train[3])\n",
    "print(y_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barrister adewale coker chamberslegal practitioners  notary publicblk  law house buildinglagosnigeriafor your kind attention              request for assistanceit is my humble pleasure to write you this letter irrespective of the fact that you do not know me  howeveri am in search of a reliable and trustworthy person that can handle a confidential transaction of this naturei am barrister adewale coker a family lawyer to our former military rulegeneral sani abacha who died suddenly in power some years ago since his untimely demise the family has suffered a lot of harassment from the regimes that succeeded him  the regime and even the present civilian government are made up of abachas enemiesrecently the wife was banned from traveling outside kano state their home state as a kind of house arrest and the eldest son still in detentionalthough a lot of money have been recovered from mrs abacha since the death of her husband by the present government theres still huge sums of money in hard currencies that we have been able to move out of the country for safe keeping to the tune of us millionthis money us million is already in north american and if you are interestedwe will prepare you as the beneficiary of the total fundsand you will share  of the total funds after clearance from the security companynote there is no risk involved in this project because l am involved as abachas confidantplease you should keep this transaction a top secret and we are prepared to do more business with you pending your approach towards this projecti await your urgent responsethanksyours faithfullybarrister adewale coker\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_test[4])\n",
    "print(y_test[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "    return result\n",
    "\n",
    "def word_stemmer(words):\n",
    "    return [stemmer.stem(o) for o in words]\n",
    "\n",
    "def word_lemmatizer(words):\n",
    "    return [lemmatizer.lemmatize(o) for o in words]\n",
    "\n",
    "def clean_token_pipeline(words):\n",
    "    cleaning_utils = [remove_stop_words,word_lemmatizer]\n",
    "    for o in cleaning_utils:\n",
    "        words = o(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.26.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train_index = [i for i,o in enumerate(y_train) if o == 1]\n",
    "non_spam_train_index = [i for i,o in enumerate(y_train) if o == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_size = len(spam_train_index)\n",
    "non_spam_size = len(non_spam_train_index)\n",
    "total_train_size = spam_size + non_spam_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/muralikorrapati/pythonPractice/MLearning/spam_filter_nttdata/Email Spam Filter.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muralikorrapati/pythonPractice/MLearning/spam_filter_nttdata/Email%20Spam%20Filter.ipynb#Y112sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m data \u001b[39m=\u001b[39m [trace]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muralikorrapati/pythonPractice/MLearning/spam_filter_nttdata/Email%20Spam%20Filter.ipynb#Y112sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m fig \u001b[39m=\u001b[39m go\u001b[39m.\u001b[39mFigure(data\u001b[39m=\u001b[39mdata, layout\u001b[39m=\u001b[39mlayout)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/muralikorrapati/pythonPractice/MLearning/spam_filter_nttdata/Email%20Spam%20Filter.ipynb#Y112sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m py\u001b[39m.\u001b[39;49miplot(fig, filename\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTargetCount\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muralikorrapati/pythonPractice/MLearning/spam_filter_nttdata/Email%20Spam%20Filter.ipynb#Y112sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m## target distribution ##\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muralikorrapati/pythonPractice/MLearning/spam_filter_nttdata/Email%20Spam%20Filter.ipynb#Y112sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m labels \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39marray([\u001b[39m\"\u001b[39m\u001b[39mSpam\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNon Spam\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/pythonPractice/MLearning/spam_filter_nttdata/.venv/lib/python3.9/site-packages/plotly/offline/offline.py:392\u001b[0m, in \u001b[0;36miplot\u001b[0;34m(figure_or_data, show_link, link_text, validate, image, filename, image_width, image_height, config, auto_play, animation_opts)\u001b[0m\n\u001b[1;32m    387\u001b[0m post_script \u001b[39m=\u001b[39m build_save_image_post_script(\n\u001b[1;32m    388\u001b[0m     image, filename, image_height, image_width, \u001b[39m\"\u001b[39m\u001b[39miplot\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m )\n\u001b[1;32m    391\u001b[0m \u001b[39m# Show figure\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m pio\u001b[39m.\u001b[39;49mshow(\n\u001b[1;32m    393\u001b[0m     figure,\n\u001b[1;32m    394\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    395\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    396\u001b[0m     auto_play\u001b[39m=\u001b[39;49mauto_play,\n\u001b[1;32m    397\u001b[0m     post_script\u001b[39m=\u001b[39;49mpost_script,\n\u001b[1;32m    398\u001b[0m     animation_opts\u001b[39m=\u001b[39;49manimation_opts,\n\u001b[1;32m    399\u001b[0m )\n",
      "File \u001b[0;32m~/pythonPractice/MLearning/spam_filter_nttdata/.venv/lib/python3.9/site-packages/plotly/io/_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         )\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "trace = go.Bar(\n",
    "    x=[\"Spam\",\"Non Spam\"],\n",
    "    y=[spam_size, non_spam_size],\n",
    "    marker=dict(\n",
    "        color=[spam_size, non_spam_size],\n",
    "        colorscale = 'Picnic',\n",
    "        reversescale = True\n",
    "    ),\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Target Count for Train Data',\n",
    "    font=dict(size=18)\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"TargetCount\")\n",
    "\n",
    "## target distribution ##\n",
    "labels = (np.array([\"Spam\", \"Non Spam\"]))\n",
    "sizes = (np.array(([spam_size,non_spam_size]))/total_train_size*100)\n",
    "\n",
    "trace = go.Pie(labels=labels, values=sizes)\n",
    "layout = go.Layout(\n",
    "    title='Train Data distribution',\n",
    "    font=dict(size=18),\n",
    "    width=600,\n",
    "    height=600,\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"usertype\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_test_index = [i for i,o in enumerate(y_test) if o == 1]\n",
    "non_spam_test_index = [i for i,o in enumerate(y_test) if o == 0]\n",
    "\n",
    "spam_size = len(spam_test_index)\n",
    "non_spam_size = len(non_spam_test_index)\n",
    "total_test_size = spam_size + non_spam_size\n",
    "\n",
    "trace = go.Bar(\n",
    "    x=[\"Spam\",\"Non Spam\"],\n",
    "    y=[spam_size, non_spam_size],\n",
    "    marker=dict(\n",
    "        color=[spam_size, non_spam_size],\n",
    "        colorscale = 'Picnic',\n",
    "        reversescale = True\n",
    "    ),\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Target Count for Test Data',\n",
    "    font=dict(size=18)\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"TargetCount\")\n",
    "\n",
    "## target distribution ##\n",
    "labels = (np.array([\"Spam\", \"Non Spam\"]))\n",
    "sizes = (np.array(([spam_size,non_spam_size]))/total_train_size*100)\n",
    "\n",
    "trace = go.Pie(labels=labels, values=sizes)\n",
    "layout = go.Layout(\n",
    "    title='Test Data Distribution',\n",
    "    font=dict(size=18),\n",
    "    width=600,\n",
    "    height=600,\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"usertype\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # You can choose a different variant\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = []\n",
    "y_train_new = []\n",
    "for i, x in enumerate(x_train):\n",
    "    if len(x) > 0:\n",
    "        x_train_new.append(x)\n",
    "        y_train_new.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_new\n",
    "y_train = y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in x_train:\n",
    "    if len(x) == 0:\n",
    "        print(f\"Empty: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_new = []\n",
    "y_test_new = []\n",
    "for i, x in enumerate(x_test):\n",
    "    if len(x) > 0:\n",
    "        x_test_new.append(x)\n",
    "        y_test_new.append(y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test_new\n",
    "y_test = y_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of x_train: {len(x_train)}\")\n",
    "print(f\"Length of y_train: {len(y_train)}\")\n",
    "print(f\"Length of x_test: {len(x_test)}\")\n",
    "print(f\"Length of y_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "max_length = 512  # Choose an appropriate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "inputs = tokenizer(x_train, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the labels as tensors\n",
    "labels = torch.tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for batch processing (optional but recommended)\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # Choose an appropriate batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model in training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and training loop (similar to the previous example)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, batch_labels = batch\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model in evaluation mode for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = pipeline(\"text-classification\",model=model,tokenizer=tokenizer)\n",
    "tokenizer_kwargs = {'truncation':True, 'max_length':max_length}\n",
    "predictions = model_pipeline(x_test,**tokenizer_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to 0 or 1\n",
    "y_predict = []\n",
    "for pred in predictions:\n",
    "    if pred[\"label\"] == \"LABEL_0\":\n",
    "        y_predict.append(0)\n",
    "    if pred[\"label\"] == \"LABEL_1\":\n",
    "        y_predict.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some sample predictions\n",
    "for example, prediction in zip(x_test[:5], y_predict[:5]):\n",
    "    print(f\"Text: {example}\\nPrediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to save the trained model for later use\n",
    "model.save_pretrained(\"distilbert_spam_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_test: {y_test[:5]}\")\n",
    "print(f\"y_predict: {y_predict[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,y_predict).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_predict)))\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predict)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_predict)))\n",
    "print(\"f1_score: {:.2f}%\".format(100 * f1_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Non Spam','Spam'], normalize=False,\n",
    "                      title='Confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
